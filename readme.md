# üéì Robo-Poet - Interfaz Acad√©mica para Generaci√≥n de Texto

**Implementaci√≥n educacional completa** de un generador de texto basado en LSTM usando TensorFlow 2.20, optimizado para GPUs NVIDIA RTX 2000 Ada con Kali Linux en WSL2.

**Nueva Arquitectura v2.0**: Interfaz acad√©mica unificada con sistema de dos fases separadas para entrenamiento intensivo y generaci√≥n de texto.

## üöÄ Inicio R√°pido

```mermaid
flowchart TD
    A[üìö Preparar Texto] --> B[üß† Entrenar Modelo]
    B --> C[üíæ Guardar Modelo]
    C --> D[üé® Generar Texto]
    
    subgraph "Fase 1: Entrenamiento"
        B --> B1[Tokenizaci√≥n]
        B1 --> B2[Secuencias LSTM]
        B2 --> B3[GPU Training]
        B3 --> B4[Validaci√≥n]
    end
    
    subgraph "Fase 2: Generaci√≥n v2.1"
        D --> D1[üöÄ R√°pida]
        D --> D2[üî¨ Laboratorio]
        D --> D3[üéØ Interactiva]
        D --> D4[‚öóÔ∏è Experimentos]
        D1 --> D5[üìù Texto Final]
        D2 --> D5
        D3 --> D5
        D4 --> D5
    end
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style D fill:#e8f5e8
    style D5 fill:#fff3e0
```

```bash
# Activar entorno conda
eval "$($HOME/miniconda3/bin/conda shell.bash hook)"       
conda activate robo-poet-gpu

# M√âTODO PRINCIPAL: Interfaz Acad√©mica (RECOMENDADO)
python robo_poet.py

# M√âTODO DIRECTO: Entrenamiento espec√≠fico
python robo_poet.py --text "The+48+Laws+Of+Power_texto.txt" --epochs 10

# M√âTODO R√ÅPIDO: Scripts auxiliares
./train --epochs 20
./generate --seed "The power of" --length 300
```

## üéØ Soluci√≥n WSL2 + GPU Implementada

**PROBLEMA RESUELTO**: El framework incluye **detecci√≥n autom√°tica de GPU para WSL2** que soluciona el error com√∫n "Cannot dlopen some GPU libraries".

### ‚úÖ Detecci√≥n Autom√°tica:
- üîç **Estrategia 1**: Detecci√≥n est√°ndar TensorFlow
- üéØ **Estrategia 2**: Acceso directo GPU (WSL2 workaround) 
- üîÑ **Estrategia 3**: Fallback modo CPU

### üéâ Resultado Esperado:
```
üéØ ¬°GPU funciona perfectamente via acceso directo!
üí° Aplicando workaround WSL2 para usar GPU
‚úÖ Todos los m√≥dulos GPU importados correctamente
```

### üîß Si Necesitas Instalar Librer√≠as CUDA:
```bash
conda install -c conda-forge cudnn libcublas libcufft libcurand libcusolver libcusparse -y
```

### üéØ Sistema Avanzado de Dos Fases v2.1

1. **üî• FASE 1**: Entrenamiento Intensivo (1+ hora) - Crea modelos robustos
2. **üé® FASE 2**: Estudio de Generaci√≥n Avanzado - 8 modos de generaci√≥n con an√°lisis completo

#### üÜï Nuevas Caracter√≠sticas FASE 2 v2.1:
- **üöÄ Generaci√≥n R√°pida**: 5 presets optimizados (Narrativa, Creativo, Experimental, Acad√©mico, Art√≠stico)
- **üî¨ Laboratorio Creativo**: Control total con generaci√≥n dirigida y variaciones
- **üéÆ Sesi√≥n Interactiva**: Comandos avanzados con estad√≠sticas en tiempo real
- **üìä Experimentos en Lote**: 4 tipos (m√∫ltiples seeds, barrido temperature, variaci√≥n longitud, matriz completa)
- **üé® Plantillas Tem√°ticas**: 5 estilos literarios predefinidos con seeds optimizados
- **üìà An√°lisis Avanzado**: Estad√≠sticas detalladas del modelo y recomendaciones de uso
- **üíæ Gesti√≥n Completa**: Guardado autom√°tico con metadata y visualizaci√≥n de archivos

## üéì Arquitectura Acad√©mica v2.0 - Sistema de Dos Fases

```mermaid
flowchart TD
    A[üéì Robo-Poet Academic Interface v2.0] --> B[üéØ Men√∫ Principal]
    
    B --> C[üî• FASE 1: Entrenamiento Intensivo]
    B --> D[üé® FASE 2: Generaci√≥n de Texto]
    B --> E[üìä Ver Modelos Disponibles]
    B --> F[üìà Monitorear Progreso]
    B --> G[‚öôÔ∏è Configuraci√≥n del Sistema]
    
    %% FASE 1 - Entrenamiento Intensivo
    C --> C1[üìÅ Selecci√≥n de Corpus]
    C1 --> C2[üéØ Configuraci√≥n de √âpocas]
    C2 --> C3[‚ö†Ô∏è Confirmaci√≥n de Entrenamiento]
    C3 --> C4[üöÄ Setup de GPU]
    C4 --> C5[üìö Preparaci√≥n de Datos]
    C5 --> C6[üß† Construcci√≥n de Modelo LSTM]
    C6 --> C7[‚ö° Entrenamiento Intensivo 1+ hora]
    C7 --> C8[üíæ Guardado Autom√°tico con Timestamp]
    C8 --> C9[üìã Metadata JSON Completa]
    
    %% FASE 2 - Generaci√≥n de Texto
    D --> D1[üìã Lista de Modelos Disponibles]
    D1 --> D2[üéØ Selecci√≥n de Modelo]
    D2 --> D3[üìÅ Carga de Modelo + Metadata]
    D3 --> D4[üé® Men√∫ de Generaci√≥n]
    
    D4 --> D4A[üìù Generaci√≥n Simple]
    D4 --> D4B[üéÆ Modo Interactivo]
    D4 --> D4C[üìä Generaci√≥n en Lote]
    
    D4A --> D5[üå°Ô∏è Control Temperature/Length]
    D4B --> D6[üîÑ Generaci√≥n Continua]
    D4C --> D7[üìà M√∫ltiples Seeds]
    
    %% Sistema de Monitoreo
    F --> F1[üîç Checkpoints Activos]
    F1 --> F2[üìÖ Estado de Entrenamientos]
    F2 --> F3[üìä TensorBoard Logs]
    
    %% Configuraci√≥n del Sistema
    G --> G1[üíª Estado de GPU]
    G1 --> G2[üéõÔ∏è Hiperpar√°metros]
    G2 --> G3[üìã Informaci√≥n del Hardware]
    
    %% Arquitectura del Sistema
    subgraph "üèóÔ∏è Arquitectura Modular"
        H[src/config.py<br/>Configuraci√≥n GPU/Modelo]
        I[src/data_processor.py<br/>Procesamiento y Generaci√≥n]
        J[src/model.py<br/>LSTM + Training + Management]
        K[robo_poet.py<br/>Interfaz Acad√©mica Unificada]
    end
    
    %% Flujo de Datos
    C9 --> L[models/robo_poet_model_TIMESTAMP.h5]
    C9 --> M[models/robo_poet_model_TIMESTAMP_metadata.json]
    L --> D1
    M --> D1
    
    %% Caracter√≠sticas T√©cnicas
    subgraph "üîß Stack Tecnol√≥gico Verificado"
        N[Kali Linux WSL2]
        O[NVIDIA RTX 2000 Ada - 8GB VRAM]
        P[TensorFlow 2.20 + CUDA 12.2]
        Q[Python 3.10 + Conda Environment]
    end
    
    %% Optimizaciones
    subgraph "‚ö° Optimizaciones RTX 2000 Ada"
        R[Mixed Precision FP16]
        S[Memory Growth Dynamic]
        T[Checkpoints Autom√°ticos]
        U[Early Stopping Inteligente]
    end
    
    style A fill:#e1f5fe,stroke:#01579b,stroke-width:3px
    style C fill:#ffebee,stroke:#c62828,stroke-width:2px
    style D fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    style F fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
    style G fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    style C7 fill:#ffcdd2,stroke:#d32f2f,stroke-width:2px
    style D6 fill:#c8e6c9,stroke:#388e3c,stroke-width:2px
    style K fill:#e1f5fe,stroke:#0277bd,stroke-width:3px
```

## üéì Marco Acad√©mico y Metodol√≥gico v2.0

Este proyecto representa una **evoluci√≥n hacia una interfaz acad√©mica unificada** que separa claramente las fases de entrenamiento y generaci√≥n, proporcionando un flujo de trabajo m√°s profesional y educativo.

### üÜï Nuevas Caracter√≠sticas v2.0

- **üéØ Interfaz Unificada**: Un solo punto de entrada (`python robo_poet.py`)
- **üì± Men√∫ Interactivo**: Navegaci√≥n acad√©mica intuitiva
- **üîÑ Sistema de Dos Fases**: Separaci√≥n clara entre entrenamiento y generaci√≥n
- **üìä Monitoreo Avanzado**: Seguimiento de progreso y modelos disponibles
- **üíæ Gesti√≥n Autom√°tica**: Guardado inteligente con metadata completa
- **üé® Generaci√≥n Vers√°til**: Modos simple, interactivo y en lote

### üß† Conceptos Acad√©micos Cubiertos

- **üèóÔ∏è Arquitecturas de Redes Neuronales**: LSTM, Embeddings, y fundamentos de Transformers
- **‚ö° Optimizaci√≥n de GPU**: Mixed Precision, Tensor Cores, y gesti√≥n de memoria VRAM
- **üìù Procesamiento de Lenguaje Natural**: Tokenizaci√≥n, vocabulario, y m√©tricas de evaluaci√≥n
- **üõ†Ô∏è Ingenier√≠a de Software**: Interfaces acad√©micas, modularizaci√≥n, y debugging sistem√°tico
- **üêß Sistemas Linux**: Configuraci√≥n de drivers, gesti√≥n de dependencias, y troubleshooting
- **üéì Metodolog√≠a Acad√©mica**: Separaci√≥n de fases, documentaci√≥n autom√°tica, y reproducibilidad

### üöÄ Template Acad√©mico para Proyectos ML

Esta implementaci√≥n v2.0 establece un **patr√≥n acad√©mico replicable** para proyectos de ML que incluye:

1. **üéØ Interfaz Acad√©mica Unificada**: Sistema de men√∫s interactivos profesionales
2. **üîÑ Metodolog√≠a de Dos Fases**: Separaci√≥n clara entre entrenamiento e inferencia
3. **üìä Monitoreo Acad√©mico**: Seguimiento de progreso y gesti√≥n de modelos
4. **üíæ Gesti√≥n Inteligente**: Guardado autom√°tico con metadata acad√©mica completa
5. **üé® Generaci√≥n Vers√°til**: M√∫ltiples modos de generaci√≥n para diferentes necesidades
6. **üì± Experiencia de Usuario**: Interfaz limpia y educativa para estudiantes

## Requisitos del Sistema

### Hardware M√≠nimo
- **GPU**: NVIDIA RTX 2000 Ada o superior (8GB VRAM)
- **RAM**: 16GB DDR4/DDR5
- **Almacenamiento**: 10GB espacio libre (modelo + datasets)
- **CPU**: Intel i5-8400 / AMD Ryzen 5 2600 o superior

### Software Requerido (Configuraci√≥n Probada)
- **OS**: Kali Linux en WSL2 (Windows 11)
- **Python**: 3.10.18 (conda environment)
- **TensorFlow**: 2.20.0
- **CUDA Toolkit**: 12.2.140 (conda-forge)
- **cuDNN**: Incluido en conda environment
- **NVIDIA Driver**: 566.24 (Windows host)
- **Conda**: 25.5.1 (Miniconda3)

### Dependencias Python (Stack Verificado)
```
tensorflow==2.20.0
cudatoolkit=12.0 (conda-forge)
cudnn (conda-forge)
numpy (latest compatible)
matplotlib (latest)
tqdm (latest)
```

## Configuraci√≥n para Kali Linux WSL2 + NVIDIA RTX 2000 Ada (M√âTODO VERIFICADO)

### Paso 1: Preparaci√≥n del Sistema WSL2

```bash
# WSL2 no requiere headers del kernel, usa driver de Windows
# Verificar que nvidia-smi funciona desde WSL2
nvidia-smi

# Debe mostrar:
# NVIDIA RTX 2000 Ada Generation Laptop GPU, Driver Version: 566.24
```

### Paso 2: Instalaci√≥n de Miniconda (REQUERIDO)

```bash
# Descargar e instalar Miniconda
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3

# Activar conda en shell actual
eval "$($HOME/miniconda3/bin/conda shell.bash hook)"

# Verificar instalaci√≥n
conda --version  # Debe mostrar: conda 25.5.1
```

### Paso 3: Crear Entorno Python con CUDA (M√âTODO QUE FUNCIONA)

```bash
# Crear entorno con Python 3.10
conda create -n robo-poet-gpu python=3.10 -y
conda activate robo-poet-gpu

# Instalar CUDA Toolkit y cuDNN desde conda-forge
conda install -c conda-forge cudatoolkit=12.0 cudnn -y

# Verificar CUDA disponible
nvcc --version
# Debe mostrar: Cuda compilation tools, release 12.2, V12.2.140
```

### Paso 4: Instalaci√≥n de TensorFlow con GPU

```bash
# Con el entorno activado (robo-poet-gpu)
pip install tensorflow==2.20.0

# Instalar dependencias adicionales
pip install numpy matplotlib tqdm tensorboard

# Verificar que GPU es detectada
python -c "import tensorflow as tf; print('GPUs:', tf.config.list_physical_devices('GPU'))"
# Debe mostrar: GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```

### Paso 5: Configuraci√≥n del Entorno Python

```bash
# Instalar Python 3.10 si no est√° disponible
sudo apt install -y python3.10 python3.10-venv python3.10-dev

# Crear directorio del proyecto
mkdir -p ~/projects/robo-poet
cd ~/projects/robo-poet

# Crear entorno virtual
python3.10 -m venv venv
source venv/bin/activate

# Actualizar pip y herramientas base
pip install --upgrade pip setuptools wheel

# Configurar pip para evitar timeouts
pip config set global.timeout 120
```

### Paso 6: Instalaci√≥n de TensorFlow con GPU

```bash
# Instalar TensorFlow
pip install tensorflow==2.15.0

# Si hay problemas con tensorrt, instalar sin las extensiones extra
pip install tensorflow==2.15.0 --no-deps
pip install absl-py astunparse flatbuffers gast google-pasta grpcio h5py keras \
    libclang ml-dtypes numpy opt-einsum packaging protobuf setuptools \
    six tensorboard tensorflow-estimator tensorflow-io-gcs-filesystem \
    termcolor typing-extensions wrapt

# Instalar dependencias del proyecto
pip install numpy==1.24.3 tqdm==4.66.1 matplotlib==3.7.2 tensorboard==2.15.0

# Verificar GPU es detectada
python -c "import tensorflow as tf; print(f'GPUs disponibles: {tf.config.list_physical_devices(\"GPU\")}')"
```

## Verificaci√≥n de Instalaci√≥n (Configuraci√≥n Actual)

### Configuraci√≥n Verificada y Funcionando
```bash
# Estado actual del sistema:
Python: 3.10.18 (conda environment)
TensorFlow: 2.20.0 
CUDA Toolkit: 12.2.140
GPU: NVIDIA RTX 2000 Ada Generation Laptop GPU
Driver: 566.24
Conda: 25.5.1
Mixed Precision: Activable para Tensor Cores
```

### Verificaci√≥n R√°pida
```bash
# Activar entorno
conda activate robo-poet-gpu

# Verificar que todo funciona
python -c "
import tensorflow as tf
import sys
print('Python:', sys.version.split()[0])
print('TensorFlow:', tf.__version__)
print('CUDA disponible:', tf.test.is_built_with_cuda())
gpus = tf.config.list_physical_devices('GPU')
print(f'GPUs detectadas: {len(gpus)}')
if gpus:
    print('‚úì ¬°Tu RTX 2000 Ada est√° funcionando!')
else:
    print('‚úó GPU no detectada')
"
```

### Script de Verificaci√≥n Completa

```bash
# Crear archivo verify_setup.py
cat > verify_setup.py << 'EOF'
#!/usr/bin/env python3
"""
Verificaci√≥n completa de instalaci√≥n GPU para robo-poet en Kali Linux
"""

import sys
import os
import subprocess

def check_system_info():
    """Verifica informaci√≥n del sistema"""
    print("=" * 60)
    print("INFORMACI√ìN DEL SISTEMA")
    print("=" * 60)
    
    # Verificar distribuci√≥n
    try:
        with open('/etc/os-release', 'r') as f:
            for line in f:
                if line.startswith('PRETTY_NAME'):
                    print(f"OS: {line.split('=')[1].strip().strip('\"')}")
                    break
    except:
        print("OS: No se pudo determinar")
    
    # Verificar kernel
    kernel = subprocess.run(['uname', '-r'], capture_output=True, text=True)
    print(f"Kernel: {kernel.stdout.strip()}")
    
    # Verificar Python
    print(f"Python: {sys.version}")

def check_nvidia_driver():
    """Verifica driver NVIDIA"""
    print("\n" + "=" * 60)
    print("NVIDIA DRIVER")
    print("=" * 60)
    
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,driver_version,memory.total',
                               '--format=csv,noheader'], capture_output=True, text=True)
        if result.returncode == 0:
            gpu_info = result.stdout.strip().split(', ')
            print(f"‚úì GPU: {gpu_info[0]}")
            print(f"‚úì Driver Version: {gpu_info[1]}")
            print(f"‚úì VRAM: {gpu_info[2]}")
        else:
            print("‚úó nvidia-smi fall√≥")
            return False
    except FileNotFoundError:
        print("‚úó nvidia-smi no encontrado - instalar driver NVIDIA")
        return False
    
    return True

def check_cuda():
    """Verifica instalaci√≥n de CUDA"""
    print("\n" + "=" * 60)
    print("CUDA TOOLKIT")
    print("=" * 60)
    
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            for line in result.stdout.split('\n'):
                if 'release' in line:
                    print(f"‚úì CUDA Version: {line.strip()}")
                    break
        else:
            print("‚úó nvcc fall√≥")
            return False
    except FileNotFoundError:
        print("‚úó nvcc no encontrado - verificar PATH o instalar CUDA")
        print("  export PATH=/usr/local/cuda-11.8/bin:$PATH")
        return False
    
    # Verificar libcudart
    cuda_lib = "/usr/local/cuda-11.8/lib64/libcudart.so"
    if os.path.exists(cuda_lib):
        print(f"‚úì CUDA Runtime Library: {cuda_lib}")
    else:
        print(f"‚úó CUDA Runtime Library no encontrada en {cuda_lib}")
        return False
    
    return True

def check_cudnn():
    """Verifica instalaci√≥n de cuDNN"""
    print("\n" + "=" * 60)
    print("cuDNN")
    print("=" * 60)
    
    cudnn_header = "/usr/local/cuda-11.8/include/cudnn_version.h"
    if os.path.exists(cudnn_header):
        print(f"‚úì cuDNN header encontrado: {cudnn_header}")
        # Intentar leer versi√≥n
        try:
            with open(cudnn_header, 'r') as f:
                for line in f:
                    if '#define CUDNN_MAJOR' in line:
                        major = line.split()[-1]
                    elif '#define CUDNN_MINOR' in line:
                        minor = line.split()[-1]
                    elif '#define CUDNN_PATCHLEVEL' in line:
                        patch = line.split()[-1]
                        print(f"‚úì cuDNN Version: {major}.{minor}.{patch}")
                        break
        except:
            print("  No se pudo leer la versi√≥n")
    else:
        print(f"‚úó cuDNN no encontrado en {cudnn_header}")
        return False
    
    return True

def check_tensorflow():
    """Verifica TensorFlow con GPU"""
    print("\n" + "=" * 60)
    print("TENSORFLOW GPU")
    print("=" * 60)
    
    try:
        import tensorflow as tf
        print(f"‚úì TensorFlow Version: {tf.__version__}")
        
        # Verificar compilaci√≥n con CUDA
        print(f"‚úì Built with CUDA: {tf.test.is_built_with_cuda()}")
        
        # Verificar GPUs
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            print(f"‚úì GPUs detectadas: {len(gpus)}")
            for gpu in gpus:
                print(f"  - {gpu.name}")
            
            # Test de operaci√≥n
            with tf.device('/GPU:0'):
                a = tf.constant([[1.0, 2.0], [3.0, 4.0]])
                b = tf.constant([[1.0, 1.0], [0.0, 1.0]])
                c = tf.matmul(a, b)
                print(f"‚úì Test de operaci√≥n matricial exitoso")
            
            # Mixed precision
            try:
                policy = tf.keras.mixed_precision.Policy('mixed_float16')
                tf.keras.mixed_precision.set_global_policy(policy)
                print("‚úì Mixed precision (FP16) disponible")
            except:
                print("‚ö† Mixed precision no disponible")
            
            return True
        else:
            print("‚úó No se detectaron GPUs")
            print("\nPosibles soluciones:")
            print("1. Verificar driver NVIDIA: nvidia-smi")
            print("2. Verificar CUDA_VISIBLE_DEVICES no est√© vac√≠o")
            print("3. Reinstalar TensorFlow: pip install --upgrade --force-reinstall tensorflow==2.15.0")
            return False
            
    except ImportError as e:
        print(f"‚úó TensorFlow no instalado: {e}")
        return False
    except Exception as e:
        print(f"‚úó Error en TensorFlow: {e}")
        return False

def main():
    """Ejecuta todas las verificaciones"""
    print("\n" + "=" * 60)
    print("VERIFICACI√ìN DE CONFIGURACI√ìN ROBO-POET")
    print("Sistema: Kali Linux + NVIDIA GPU")
    print("=" * 60)
    
    checks = [
        ("Sistema", check_system_info),
        ("NVIDIA Driver", check_nvidia_driver),
        ("CUDA Toolkit", check_cuda),
        ("cuDNN", check_cudnn),
        ("TensorFlow GPU", check_tensorflow)
    ]
    
    results = []
    for name, check_func in checks:
        try:
            result = check_func()
            if result is not False:
                results.append((name, True))
            else:
                results.append((name, False))
        except Exception as e:
            print(f"Error en {name}: {e}")
            results.append((name, False))
    
    # Resumen
    print("\n" + "=" * 60)
    print("RESUMEN DE VERIFICACI√ìN")
    print("=" * 60)
    
    all_passed = True
    for name, passed in results:
        if name != "Sistema":  # Sistema no retorna True/False
            status = "‚úì PASS" if passed else "‚úó FAIL"
            print(f"{name:20} {status}")
            if not passed:
                all_passed = False
    
    if all_passed:
        print("\n‚úì ¬°Configuraci√≥n completa exitosa!")
        print("  Puedes comenzar a usar robo-poet")
    else:
        print("\n‚úó Hay componentes que requieren atenci√≥n")
        print("  Revisa los mensajes anteriores para soluciones")
    
    return 0 if all_passed else 1

if __name__ == "__main__":
    sys.exit(main())
EOF

# Ejecutar verificaci√≥n
chmod +x verify_setup.py
python verify_setup.py
```

## üíº Uso de la Interfaz Acad√©mica v2.0

### üóÇÔ∏è Estructura del Proyecto (Limpia y Organizada)

```
robo-poet/
‚îú‚îÄ‚îÄ üìÅ src/                           # M√≥dulos del sistema
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                   # Inicializaci√≥n del paquete
‚îÇ   ‚îú‚îÄ‚îÄ config.py                     # Configuraci√≥n GPU y modelo
‚îÇ   ‚îú‚îÄ‚îÄ data_processor.py             # Procesamiento y generaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ model.py                      # LSTM + Training + Management
‚îÇ   ‚îú‚îÄ‚îÄ robo_train.py                 # Script de entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ robo_generate.py              # Script de generaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ train_wrapper.sh              # Wrapper para entorno GPU
‚îú‚îÄ‚îÄ üìÅ models/                        # Modelos entrenados
‚îÇ   ‚îú‚îÄ‚îÄ robo_poet_model_TIMESTAMP.h5  # Modelos con timestamp
‚îÇ   ‚îî‚îÄ‚îÄ *_metadata.json               # Metadata acad√©mica completa
‚îú‚îÄ‚îÄ üìÅ logs/                          # TensorBoard logs
‚îú‚îÄ‚îÄ üìÑ robo_poet.py                   # üéØ INTERFAZ ACAD√âMICA PRINCIPAL
‚îú‚îÄ‚îÄ üìÑ train                          # Launcher entrenamiento r√°pido
‚îú‚îÄ‚îÄ üìÑ generate                       # Launcher generaci√≥n r√°pida
‚îú‚îÄ‚îÄ üìÑ The+48+Laws+Of+Power_texto.txt # Corpus de ejemplo
‚îú‚îÄ‚îÄ üìÑ CLAUDE.md                      # Metodolog√≠a acad√©mica
‚îî‚îÄ‚îÄ üìÑ readme.md                      # Documentaci√≥n completa
```

### üéÆ Uso de la Interfaz Acad√©mica

#### üöÄ Inicio del Sistema

```bash
# Activar entorno conda
eval "$($HOME/miniconda3/bin/conda shell.bash hook)"
conda activate robo-poet-gpu

# Configurar variables CUDA
export CUDA_HOME=$CONDA_PREFIX
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH

# Ejecutar interfaz acad√©mica unificada
python robo_poet.py
```

#### üéØ Flujo de Trabajo Acad√©mico

**Primera Sesi√≥n (Entrenamiento):**
1. Ejecuta `python robo_poet.py`
2. Selecciona `1. üî• FASE 1: Entrenamiento Intensivo`
3. Configura archivo de texto y √©pocas
4. Confirma entrenamiento (1+ hora)
5. El sistema guarda autom√°ticamente el modelo

**Sesiones Posteriores (Generaci√≥n):**
1. Ejecuta `python robo_poet.py`
2. Selecciona `2. üé® FASE 2: Generaci√≥n de Texto`
3. Elige modelo pre-entrenado
4. Selecciona modo de generaci√≥n (simple/interactivo/lote)

**Monitoreo:**
- Opci√≥n `4. üìà Monitorear Progreso` para ver entrenamientos activos
- Opci√≥n `3. üìä Ver Modelos` para gestionar modelos disponibles

#### üìä Monitoreo con TensorBoard

```bash
# En terminal separada (mientras entrenas)
tensorboard --logdir logs --port 6006 --bind_all

# Acceder desde navegador
# http://localhost:6006
# O desde otra m√°quina: http://[IP-DE-KALI]:6006
```

#### üéì Caracter√≠sticas Acad√©micas Destacadas

- **üì± Interfaz Unificada**: Todo desde un solo comando
- **üîÑ Separaci√≥n de Fases**: Entrenamiento vs. Generaci√≥n claramente diferenciados
- **üíæ Gesti√≥n Autom√°tica**: Guardado inteligente con timestamps y metadata
- **üìä Monitoreo Avanzado**: Seguimiento de progreso y modelos disponibles
- **üé® Generaci√≥n Vers√°til**: M√∫ltiples modos (simple, interactivo, lote)
- **‚öôÔ∏è Configuraci√≥n Transparente**: Informaci√≥n del sistema siempre accesible

## Preparaci√≥n de Datos

### Descarga de Datasets de Ejemplo

```bash
# Literatura en espa√±ol (Quijote)
wget https://www.gutenberg.org/files/2000/2000-0.txt -O data/raw/quijote.txt

# C√≥digo Python
git clone --depth 1 https://github.com/tensorflow/models.git temp/
find temp/ -name "*.py" -exec cat {} \; > data/raw/python_code.txt
rm -rf temp/

# Conversi√≥n de encoding si es necesario
iconv -f ISO-8859-1 -t UTF-8 data/raw/quijote.txt -o data/raw/quijote_utf8.txt
```

## Optimizaci√≥n de Rendimiento en Linux

### Configuraci√≥n del Sistema

```bash
# Configuraci√≥n de GPU para m√°ximo rendimiento
sudo nvidia-smi -pm 1  # Persistence mode
sudo nvidia-smi -pl 140  # Power limit (ajustar seg√∫n GPU)

# Deshabilitar CPU frequency scaling para consistencia
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# Aumentar l√≠mites del sistema
ulimit -n 65536  # File descriptors
ulimit -u 32768  # Procesos de usuario
```

### Script de Configuraci√≥n GPU

```bash
# Crear scripts/setup_gpu.sh
cat > scripts/setup_gpu.sh << 'EOF'
#!/bin/bash
# Configuraci√≥n √≥ptima de GPU para entrenamiento

echo "Configurando GPU para m√°ximo rendimiento..."

# Verificar GPU disponible
if ! nvidia-smi &> /dev/null; then
    echo "Error: nvidia-smi no disponible"
    exit 1
fi

# Establecer persistence mode
sudo nvidia-smi -pm 1

# Configurar power limit (ajustar seg√∫n GPU)
# RTX 2000 Ada t√≠picamente 70-140W
sudo nvidia-smi -pl 140

# Configurar compute mode exclusivo (opcional)
# sudo nvidia-smi -c EXCLUSIVE_PROCESS

# Mostrar configuraci√≥n actual
nvidia-smi --query-gpu=name,persistence_mode,power.limit --format=csv

echo "Configuraci√≥n completa"
EOF

chmod +x scripts/setup_gpu.sh
./scripts/setup_gpu.sh
```

### Variables de Entorno Optimizadas

```bash
# A√±adir a ~/.bashrc o ~/.zshrc
export TF_CPP_MIN_LOG_LEVEL=2  # Reducir verbosidad
export TF_FORCE_GPU_ALLOW_GROWTH=true  # Crecimiento din√°mico de memoria
export TF_GPU_THREAD_MODE=gpu_private  # Threading optimizado
export TF_CUDNN_DETERMINISTIC=1  # Reproducibilidad
export CUDA_CACHE_MAXSIZE=4294967296  # 4GB cache para kernels

# Optimizaciones XLA
export TF_XLA_FLAGS=--tf_xla_auto_jit=2
export XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda-11.8
```

## Troubleshooting en Kali Linux + WSL2

### üîß Problema Principal: "Cannot dlopen some GPU libraries" en WSL2

**PROBLEMA M√ÅS COM√öN**: TensorFlow no detecta GPU en WSL2 aunque funciona perfectamente.

#### ‚úÖ SOLUCI√ìN IMPLEMENTADA (Ya incluida en el c√≥digo):

El framework incluye **detecci√≥n autom√°tica WSL2** que:
1. Intenta detecci√≥n est√°ndar de TensorFlow
2. Si falla, usa acceso directo a GPU (WSL2 workaround)
3. Configura autom√°ticamente el entorno para usar GPU

**Salida esperada cuando funciona:**
```
üéØ ¬°GPU funciona perfectamente via acceso directo!
üí° Aplicando workaround WSL2 para usar GPU
‚úÖ Todos los m√≥dulos GPU importados correctamente
```

#### üîß Si a√∫n hay problemas, instalar librer√≠as CUDA:

```bash
# Activar entorno conda
conda activate robo-poet-gpu

# Instalar todas las librer√≠as CUDA necesarias
conda install -c conda-forge cudnn libcublas libcufft libcurand libcusolver libcusparse -y

# Verificar instalaci√≥n
python -c "
import tensorflow as tf
with tf.device('/GPU:0'):
    print('‚úÖ GPU funcional:', tf.reduce_sum([1,2,3]))
"
```

### Error: CUDA out of memory

```bash
# Soluci√≥n 1: Reiniciar GPU en WSL2
# En PowerShell como administrador:
wsl --shutdown
# Luego reiniciar WSL2

# Soluci√≥n 2: Reducir batch size en interfaz acad√©mica
# El sistema ajusta autom√°ticamente para 8GB VRAM

# Soluci√≥n 3: Monitorear uso
nvidia-smi
```

### Error: Variables de entorno incorrectas

```bash
# El sistema configura autom√°ticamente, pero si hay problemas:
export CUDA_HOME=$CONDA_PREFIX
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$CONDA_PREFIX/lib64:$LD_LIBRARY_PATH
export CUDA_VISIBLE_DEVICES=0
export TF_FORCE_GPU_ALLOW_GROWTH=true

# Verificar configuraci√≥n
echo "CONDA_PREFIX: $CONDA_PREFIX"
echo "CUDA_HOME: $CUDA_HOME"
```

### Verificaci√≥n de Estado del Sistema

```bash
# 1. Verificar GPU visible desde Windows
nvidia-smi

# 2. Verificar entorno conda activo
conda info --envs
# Debe mostrar * junto a robo-poet-gpu

# 3. Verificar librer√≠as CUDA
find $CONDA_PREFIX/lib -name "libcu*.so*" | head -5

# 4. Test completo
python robo_poet.py --help
# Debe mostrar: "‚úÖ GPU funciona perfectamente via acceso directo"
```

### Monitoreo de Recursos en WSL2

```bash
# GPU monitoring b√°sico
nvidia-smi

# Monitoreo continuo (en terminal separada)
watch -n 1 nvidia-smi

# Durante entrenamiento, verificar uso de GPU
nvidia-smi dmon -s u -d 1
```

## üìä Benchmarks Esperados (RTX 2000 Ada + Interfaz v2.0)

Con configuraci√≥n √≥ptima en RTX 2000 Ada (Kali Linux):

| M√©trica | Valor | Contexto Acad√©mico |
|---------|-------|--------------------|
| Tokens/segundo (training) | 18,000-22,000 | FASE 1: Entrenamiento Intensivo |
| Batch size m√°ximo | 128 (seq_len=40) | Optimizado para 8GB VRAM |
| Tiempo/√©poca (10MB dataset) | 5-8 minutos | ~50 √©pocas = 4-7 horas |
| Memoria GPU utilizada | 6.5-7.8 GB | Monitoreable desde la interfaz |
| Temperatura GPU | 65-75¬∞C | Visible en configuraci√≥n del sistema |
| Power draw | 120-140W | Rendimiento m√°ximo RTX 2000 Ada |
| **Tiempo FASE 1 completa** | **1-3 horas** | **Entrenamiento intensivo acad√©mico** |
| **Tiempo FASE 2 (generaci√≥n)** | **< 1 segundo** | **Inferencia instant√°nea** |

## Seguridad y Mejores Pr√°cticas

### Aislamiento del Entorno

```bash
# Usar entornos virtuales siempre
python -m venv venv --system-site-packages=false

# No ejecutar con sudo a menos que sea necesario
# Usar --user para instalaciones locales si no hay venv
pip install --user tensorflow==2.15.0
```

### Backup de Modelos

```bash
# Script de backup autom√°tico
cat > scripts/backup_model.sh << 'EOF'
#!/bin/bash
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="backups/model_${TIMESTAMP}"
mkdir -p "$BACKUP_DIR"
cp -r models/final/* "$BACKUP_DIR/"
echo "Backup creado en $BACKUP_DIR"
EOF

chmod +x scripts/backup_model.sh
```

## üìö Recursos Adicionales

### üìñ Documentaci√≥n T√©cnica
- [TensorFlow GPU Support](https://www.tensorflow.org/install/gpu)
- [NVIDIA CUDA on Linux](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/)
- [Kali Linux NVIDIA Drivers](https://www.kali.org/docs/general-use/install-nvidia-drivers-on-kali-linux/)
- **CLAUDE.md**: Metodolog√≠a acad√©mica completa incluida en el proyecto

### üõ†Ô∏è Herramientas de Monitoreo
- `nvtop`: Monitor GPU interactivo
- `gpustat`: Estado GPU en terminal
- `tensorflow-profiler`: Profiling detallado
- **Interfaz v2.0**: Monitoreo integrado en el men√∫ principal

### üë• Comunidad Acad√©mica
- TensorFlow Forum: discuss.tensorflow.org
- NVIDIA Developer Forums: forums.developer.nvidia.com
- Kali Linux Forums: forums.kali.org
- **Issues**: Para reportar problemas o mejoras del proyecto

### üéØ Casos de Uso Acad√©micos
- **Cursos de NLP**: Implementaci√≥n pr√°ctica de LSTM para generaci√≥n de texto
- **Investigaci√≥n en ML**: Base s√≥lida para experimentos con arquitecturas neuronales
- **Tesis de Grado**: Template completo para proyectos de texto generativo
- **Workshops**: Interfaz acad√©mica lista para demostraciones educativas

## üéì Evoluci√≥n del Proyecto

### v1.0 ‚Üí v2.0: Transformaci√≥n Acad√©mica

**v1.0 (Anterior):**
- Scripts separados para entrenamiento y generaci√≥n
- CLI con argumentos complejos
- Gesti√≥n manual de modelos

**v2.0 (Actual):**
- ‚úÖ **Interfaz acad√©mica unificada**
- ‚úÖ **Sistema de dos fases bien definidas**
- ‚úÖ **Men√∫s interactivos profesionales**
- ‚úÖ **Gesti√≥n autom√°tica de modelos**
- ‚úÖ **Monitoreo de progreso integrado**
- ‚úÖ **Root folder limpio y organizado**

### üöÄ Pr√≥ximas Mejoras

- üìä Integraci√≥n de m√©tricas de evaluaci√≥n autom√°tica
- üéØ Fine-tuning de modelos pre-entrenados
- üì± Interfaz web acad√©mica opcional
- üîÑ Export a diferentes formatos (ONNX, TensorFlow Lite)

## üî¨ Documentaci√≥n T√©cnica: Soluci√≥n WSL2 GPU Detection

### Problema T√©cnico Resuelto

**Situaci√≥n**: En WSL2 con Kali Linux, TensorFlow 2.20.0 frecuentemente falla al detectar GPUs NVIDIA usando `tf.config.list_physical_devices('GPU')`, aunque las operaciones GPU funcionan perfectamente.

**Root Cause**: Incompatibilidad entre el driver NVIDIA de Windows y la detecci√≥n de dispositivos f√≠sicos en el contexto de WSL2.

### Soluci√≥n Implementada

#### 1. Detecci√≥n Multi-Estrategia (`detect_gpu_for_wsl2()`)

```python
def detect_gpu_for_wsl2():
    # Estrategia 1: Detecci√≥n est√°ndar
    tf_gpus = tf.config.list_physical_devices('GPU')
    if tf_gpus:
        return True, tf  # Funciona normalmente
    
    # Estrategia 2: Acceso directo (WSL2 workaround)
    try:
        with tf.device('/GPU:0'):
            test_tensor = tf.constant([1.0, 2.0, 3.0])
            result = tf.reduce_sum(test_tensor)
        return True, tf  # GPU funciona aunque no se detecta
    except Exception:
        return False, tf  # GPU realmente no disponible
```

#### 2. Configuraci√≥n GPU Unificada (`src/config.py`)

```python
def setup_gpu() -> bool:
    # Misma l√≥gica aplicada a m√≥dulos de entrenamiento
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if not gpus:
        # WSL2 workaround: test directo
        with tf.device('/GPU:0'):
            test_tensor = tf.constant([1.0])
        # GPU funciona, proceder con configuraci√≥n
```

#### 3. Variables de Entorno Optimizadas

```python
# Configuraci√≥n autom√°tica en inicio
os.environ['CUDA_HOME'] = conda_prefix
os.environ['LD_LIBRARY_PATH'] = f'{conda_prefix}/lib:{conda_prefix}/lib64'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
```

### Librer√≠as CUDA Requeridas

Para TensorFlow 2.20.0 en conda environment:

```bash
# Core CUDA libraries
- libcudnn: 9.12.0+ (Deep Learning operations)
- libcublas: 12.9.1+ (Linear algebra)
- libcufft: 11.4.1+ (Fast Fourier Transform)
- libcurand: 10.3.10+ (Random number generation) 
- libcusolver: 11.7.5+ (Linear algebra solver)
- libcusparse: 12.5.10+ (Sparse matrix operations)
```

### Ventajas de Esta Implementaci√≥n

1. **Compatibilidad Total**: Funciona tanto en sistemas con detecci√≥n est√°ndar como en WSL2
2. **Degradaci√≥n Elegante**: Fallback autom√°tico a CPU si GPU no disponible
3. **Diagn√≥stico Autom√°tico**: Identifica y reporta problemas espec√≠ficos
4. **Zero Configuration**: El usuario no necesita configurar nada manualmente

### Casos de Uso Verificados

‚úÖ **Funciona en**: WSL2 + Kali Linux + RTX 2000 Ada + TensorFlow 2.20.0  
‚úÖ **Funciona en**: Linux nativo + NVIDIA GPUs  
‚úÖ **Funciona en**: Sistemas sin GPU (modo CPU)  
‚úÖ **Funciona en**: Entornos conda y virtualenv  

## üìú Licencia

MIT License - Proyecto educacional de c√≥digo abierto para estudiantes de ML.

## üôè Agradecimientos

**Soluci√≥n WSL2**: Desarrollada espec√≠ficamente para resolver incompatibilidades de detecci√≥n GPU en entornos WSL2 con NVIDIA drivers. Esta implementaci√≥n permite el uso completo de aceleraci√≥n GPU en Windows Subsystem for Linux sin requerir configuraci√≥n manual.