#!/usr/bin/env python3
"""
üõ°Ô∏è ANTI-SATURATION SYSTEM - M√ìDULO 2 TASK 4.1
Creado por Bernard Orozco bajo tutela de Aslan

Sistema de prevenci√≥n proactiva de saturaci√≥n de gates LSTM.
Monitorea en tiempo real durante entrenamiento y toma acciones correctivas
ANTES de que el modelo se da√±e.

CARACTER√çSTICAS:
- Monitoreo cada N batches
- Reducci√≥n autom√°tica de learning rate
- Gradient clipping adaptativo
- Checkpoints de emergencia
- Early stopping inteligente
"""

import os
import numpy as np
import tensorflow as tf
from pathlib import Path
from typing import Dict, List, Optional, Any
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')


class AntiSaturationCallback(tf.keras.callbacks.Callback):
    """
    üõ°Ô∏è CALLBACK DE PREVENCI√ìN DE SATURACI√ìN
    
    Implementa el sistema especificado en specs.md Task 4.1:
    - Monitor gate means cada 10 batches
    - Si any_gate_mean < 0.1 or > 0.9: acci√≥n correctiva
    - Si gradient_norm < 1e-5: DETENER entrenamiento
    
    Created by Bernard Orozco
    """
    
    def __init__(self, 
                 check_frequency: int = 10,
                 gate_threshold_low: float = 0.1,
                 gate_threshold_high: float = 0.9,
                 gradient_threshold: float = 1e-5,
                 lr_reduction_factor: float = 0.5,
                 auto_save: bool = True,
                 verbose: int = 1):
        """
        Inicializar sistema anti-saturaci√≥n
        
        Args:
            check_frequency: Revisar cada N batches
            gate_threshold_low: Umbral inferior para gates
            gate_threshold_high: Umbral superior para gates
            gradient_threshold: Umbral m√≠nimo de gradiente
            lr_reduction_factor: Factor de reducci√≥n de LR
            auto_save: Guardar checkpoints autom√°ticamente
            verbose: Nivel de verbosidad (0=silencioso, 1=normal, 2=debug)
        """
        super().__init__()
        
        self.check_frequency = check_frequency
        self.gate_threshold_low = gate_threshold_low
        self.gate_threshold_high = gate_threshold_high
        self.gradient_threshold = gradient_threshold
        self.lr_reduction_factor = lr_reduction_factor
        self.auto_save = auto_save
        self.verbose = verbose
        
        # Estado interno
        self.batch_count = 0
        self.saturation_warnings = []
        self.emergency_saves = []
        self.lr_reductions = []
        self.gradient_history = []
        self.gate_history = []
        
        # Flag para detener entrenamiento
        self.stop_training = False
        
        if self.verbose:
            print("üõ°Ô∏è ANTI-SATURATION SYSTEM ACTIVADO")
            print(f"   Check frequency: cada {check_frequency} batches")
            print(f"   Gate thresholds: [{gate_threshold_low}, {gate_threshold_high}]")
            print(f"   Gradient threshold: {gradient_threshold}")
    
    def on_train_batch_end(self, batch, logs=None):
        """
        Monitoreo al final de cada batch
        """
        self.batch_count += 1
        
        # Solo revisar cada N batches
        if self.batch_count % self.check_frequency != 0:
            return
        
        # Realizar diagn√≥stico completo
        diagnostics = self._run_diagnostics()
        
        # Tomar acciones seg√∫n diagn√≥stico
        self._take_corrective_actions(diagnostics)
        
        # Logging si verbose
        if self.verbose >= 2:
            self._log_status(diagnostics)
    
    def _run_diagnostics(self) -> Dict[str, Any]:
        """
        Ejecutar diagn√≥stico completo del modelo
        
        Returns:
            diagnostics: Diccionario con m√©tricas y problemas detectados
        """
        diagnostics = {
            'batch': self.batch_count,
            'timestamp': datetime.now().isoformat(),
            'problems': [],
            'gate_stats': {},
            'gradient_stats': {}
        }
        
        # Analizar gates LSTM
        for layer in self.model.layers:
            if 'lstm' in layer.name.lower():
                gate_stats = self._analyze_lstm_gates(layer)
                diagnostics['gate_stats'][layer.name] = gate_stats
                
                # Detectar problemas en gates
                for gate_name, stats in gate_stats.items():
                    mean_val = stats['mean']
                    
                    if mean_val < self.gate_threshold_low:
                        problem = {
                            'type': 'gate_saturated_low',
                            'layer': layer.name,
                            'gate': gate_name,
                            'value': mean_val,
                            'severity': 'critical' if mean_val < 0.05 else 'warning'
                        }
                        diagnostics['problems'].append(problem)
                        
                    elif mean_val > self.gate_threshold_high:
                        problem = {
                            'type': 'gate_saturated_high',
                            'layer': layer.name,
                            'gate': gate_name,
                            'value': mean_val,
                            'severity': 'critical' if mean_val > 0.95 else 'warning'
                        }
                        diagnostics['problems'].append(problem)
        
        # Analizar gradientes (si es posible)
        gradient_norms = self._analyze_gradients()
        if gradient_norms:
            diagnostics['gradient_stats'] = gradient_norms
            
            # Detectar vanishing gradients
            min_grad = min(gradient_norms.values())
            if min_grad < self.gradient_threshold:
                problem = {
                    'type': 'vanishing_gradients',
                    'min_gradient': min_grad,
                    'severity': 'critical'
                }
                diagnostics['problems'].append(problem)
        
        # Guardar en historial
        self.gate_history.append(diagnostics['gate_stats'])
        self.gradient_history.append(diagnostics['gradient_stats'])
        
        return diagnostics
    
    def _analyze_lstm_gates(self, layer) -> Dict[str, Dict[str, float]]:
        """
        Analizar gates de una capa LSTM
        
        Args:
            layer: Capa LSTM a analizar
            
        Returns:
            gate_stats: Estad√≠sticas por gate
        """
        gate_stats = {}
        
        try:
            weights = layer.get_weights()
            if len(weights) >= 3:
                bias = weights[2]  # Los bias revelan el estado de los gates
                units = layer.units
                
                # Extraer bias de cada gate
                gate_stats['input_gate'] = {
                    'mean': float(np.mean(bias[:units])),
                    'std': float(np.std(bias[:units])),
                    'min': float(np.min(bias[:units])),
                    'max': float(np.max(bias[:units]))
                }
                
                gate_stats['forget_gate'] = {
                    'mean': float(np.mean(bias[units:units*2])),
                    'std': float(np.std(bias[units:units*2])),
                    'min': float(np.min(bias[units:units*2])),
                    'max': float(np.max(bias[units:units*2]))
                }
                
                gate_stats['cell_gate'] = {
                    'mean': float(np.mean(bias[units*2:units*3])),
                    'std': float(np.std(bias[units*2:units*3])),
                    'min': float(np.min(bias[units*2:units*3])),
                    'max': float(np.max(bias[units*2:units*3]))
                }
                
                gate_stats['output_gate'] = {
                    'mean': float(np.mean(bias[units*3:units*4])),
                    'std': float(np.std(bias[units*3:units*4])),
                    'min': float(np.min(bias[units*3:units*4])),
                    'max': float(np.max(bias[units*3:units*4]))
                }
                
        except Exception as e:
            if self.verbose >= 2:
                print(f"‚ö†Ô∏è Error analizando gates de {layer.name}: {e}")
        
        return gate_stats
    
    def _analyze_gradients(self) -> Dict[str, float]:
        """
        Analizar normas de gradientes (aproximaci√≥n)
        
        Returns:
            gradient_norms: Normas por capa
        """
        gradient_norms = {}
        
        # Nota: En un callback no tenemos acceso directo a gradientes
        # Esta es una aproximaci√≥n basada en cambios de pesos
        # En producci√≥n, usar un GradientTape custom
        
        try:
            for layer in self.model.layers:
                if layer.trainable and len(layer.get_weights()) > 0:
                    # Aproximar gradient norm basado en learning rate y cambios
                    weights = layer.get_weights()[0]
                    weight_norm = np.linalg.norm(weights)
                    
                    # Heur√≠stica: si los pesos no cambian, gradientes son ~0
                    gradient_norms[layer.name] = weight_norm * 1e-6  # Aproximaci√≥n
                    
        except Exception as e:
            if self.verbose >= 2:
                print(f"‚ö†Ô∏è Error analizando gradientes: {e}")
        
        return gradient_norms
    
    def _take_corrective_actions(self, diagnostics: Dict[str, Any]):
        """
        Tomar acciones correctivas seg√∫n los problemas detectados
        
        Args:
            diagnostics: Resultados del diagn√≥stico
        """
        if not diagnostics['problems']:
            return
        
        # Contar severidad
        critical_count = sum(1 for p in diagnostics['problems'] if p['severity'] == 'critical')
        warning_count = sum(1 for p in diagnostics['problems'] if p['severity'] == 'warning')
        
        if self.verbose:
            print(f"\nüö® ALERTA ANTI-SATURACI√ìN (Batch {self.batch_count}):")
            print(f"   Problemas cr√≠ticos: {critical_count}")
            print(f"   Warnings: {warning_count}")
        
        # Acciones seg√∫n severidad
        if critical_count > 0:
            self._handle_critical_problems(diagnostics)
        elif warning_count > 0:
            self._handle_warning_problems(diagnostics)
    
    def _handle_critical_problems(self, diagnostics: Dict[str, Any]):
        """
        Manejar problemas cr√≠ticos (acciones agresivas)
        """
        if self.verbose:
            print("   üî¥ PROBLEMAS CR√çTICOS DETECTADOS")
        
        # 1. Guardar checkpoint de emergencia
        if self.auto_save:
            checkpoint_path = self._save_emergency_checkpoint()
            self.emergency_saves.append({
                'batch': self.batch_count,
                'path': checkpoint_path,
                'problems': diagnostics['problems']
            })
            if self.verbose:
                print(f"   üíæ Checkpoint de emergencia: {checkpoint_path}")
        
        # 2. Reducir learning rate agresivamente
        current_lr = float(self.model.optimizer.learning_rate)
        new_lr = current_lr * (self.lr_reduction_factor ** 2)  # Reducci√≥n doble
        
        tf.keras.backend.set_value(
            self.model.optimizer.learning_rate,
            new_lr
        )
        
        self.lr_reductions.append({
            'batch': self.batch_count,
            'old_lr': current_lr,
            'new_lr': new_lr,
            'reason': 'critical_saturation'
        })
        
        if self.verbose:
            print(f"   üìâ Learning rate: {current_lr:.2e} ‚Üí {new_lr:.2e}")
        
        # 3. Aplicar gradient clipping m√°s agresivo
        if hasattr(self.model.optimizer, 'clipnorm'):
            self.model.optimizer.clipnorm = 0.5  # Muy conservador
            if self.verbose:
                print(f"   ‚úÇÔ∏è Gradient clipping: 0.5 (agresivo)")
        
        # 4. Verificar si debemos detener
        vanishing_gradient = any(
            p['type'] == 'vanishing_gradients' for p in diagnostics['problems']
        )
        
        if vanishing_gradient:
            if self.verbose:
                print("   üíÄ VANISHING GRADIENTS - DETENIENDO ENTRENAMIENTO")
            self.model.stop_training = True
            self.stop_training = True
        
        # Registrar warning
        self.saturation_warnings.append({
            'batch': self.batch_count,
            'type': 'critical',
            'diagnostics': diagnostics
        })
    
    def _handle_warning_problems(self, diagnostics: Dict[str, Any]):
        """
        Manejar problemas de advertencia (acciones moderadas)
        """
        if self.verbose:
            print("   üü° WARNINGS DETECTADOS")
        
        # 1. Reducir learning rate moderadamente
        current_lr = float(self.model.optimizer.learning_rate)
        new_lr = current_lr * self.lr_reduction_factor
        
        tf.keras.backend.set_value(
            self.model.optimizer.learning_rate,
            new_lr
        )
        
        self.lr_reductions.append({
            'batch': self.batch_count,
            'old_lr': current_lr,
            'new_lr': new_lr,
            'reason': 'warning_saturation'
        })
        
        if self.verbose:
            print(f"   üìâ Learning rate: {current_lr:.2e} ‚Üí {new_lr:.2e}")
        
        # 2. Ajustar gradient clipping
        if hasattr(self.model.optimizer, 'clipnorm'):
            self.model.optimizer.clipnorm = 0.8
            if self.verbose:
                print(f"   ‚úÇÔ∏è Gradient clipping: 0.8 (moderado)")
        
        # Registrar warning
        self.saturation_warnings.append({
            'batch': self.batch_count,
            'type': 'warning',
            'diagnostics': diagnostics
        })
    
    def _save_emergency_checkpoint(self) -> str:
        """
        Guardar checkpoint de emergencia
        
        Returns:
            checkpoint_path: Ruta del checkpoint guardado
        """
        checkpoint_dir = Path("emergency_checkpoints")
        checkpoint_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_name = f"emergency_batch_{self.batch_count}_{timestamp}.keras"
        checkpoint_path = checkpoint_dir / checkpoint_name
        
        try:
            self.model.save(str(checkpoint_path))
            return str(checkpoint_path)
        except Exception as e:
            if self.verbose:
                print(f"   ‚ùå Error guardando checkpoint: {e}")
            return ""
    
    def _log_status(self, diagnostics: Dict[str, Any]):
        """
        Log detallado del estado (modo debug)
        """
        print(f"\nüìä STATUS (Batch {self.batch_count}):")
        
        # Gates status
        for layer_name, gates in diagnostics['gate_stats'].items():
            print(f"   {layer_name}:")
            for gate_name, stats in gates.items():
                status = "üü¢"
                if stats['mean'] < self.gate_threshold_low or stats['mean'] > self.gate_threshold_high:
                    status = "üî¥"
                print(f"     {status} {gate_name}: Œº={stats['mean']:.3f}")
        
        # Gradient status
        if diagnostics['gradient_stats']:
            min_grad = min(diagnostics['gradient_stats'].values())
            max_grad = max(diagnostics['gradient_stats'].values())
            print(f"   Gradients: min={min_grad:.2e}, max={max_grad:.2e}")
    
    def on_train_end(self, logs=None):
        """
        Al finalizar el entrenamiento, generar reporte
        """
        if self.verbose:
            print("\nüõ°Ô∏è ANTI-SATURATION SYSTEM - REPORTE FINAL")
            print("=" * 50)
            print(f"   Total warnings: {len(self.saturation_warnings)}")
            print(f"   Emergency saves: {len(self.emergency_saves)}")
            print(f"   LR reductions: {len(self.lr_reductions)}")
            
            if self.stop_training:
                print("   üõë Entrenamiento detenido por sistema de seguridad")
        
        # Guardar reporte completo
        self._save_final_report()
    
    def _save_final_report(self):
        """
        Guardar reporte final del monitoreo
        """
        report = {
            'summary': {
                'total_batches': self.batch_count,
                'total_warnings': len(self.saturation_warnings),
                'emergency_saves': len(self.emergency_saves),
                'lr_reductions': len(self.lr_reductions),
                'training_stopped': self.stop_training
            },
            'configuration': {
                'check_frequency': self.check_frequency,
                'gate_threshold_low': self.gate_threshold_low,
                'gate_threshold_high': self.gate_threshold_high,
                'gradient_threshold': self.gradient_threshold,
                'lr_reduction_factor': self.lr_reduction_factor
            },
            'warnings': self.saturation_warnings,
            'emergency_saves': self.emergency_saves,
            'lr_reductions': self.lr_reductions,
            'timestamp': datetime.now().isoformat()
        }
        
        report_path = f"antisaturation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(report_path, 'w') as f:
                json.dump(report, f, indent=2, default=str)
            
            if self.verbose:
                print(f"   üìã Reporte guardado: {report_path}")
        except Exception as e:
            if self.verbose:
                print(f"   ‚ùå Error guardando reporte: {e}")


# Funci√≥n de conveniencia
def create_antisaturation_callback(**kwargs) -> AntiSaturationCallback:
    """
    Crear callback anti-saturaci√≥n con configuraci√≥n por defecto
    
    Created by Bernard Orozco
    """
    default_config = {
        'check_frequency': 10,
        'gate_threshold_low': 0.1,
        'gate_threshold_high': 0.9,
        'gradient_threshold': 1e-5,
        'lr_reduction_factor': 0.5,
        'auto_save': True,
        'verbose': 1
    }
    
    # Actualizar con kwargs proporcionados
    default_config.update(kwargs)
    
    return AntiSaturationCallback(**default_config)