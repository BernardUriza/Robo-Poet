#!/usr/bin/env python3
"""
Phase 2 Generation Interface for Robo-Poet Framework

Advanced text generation interface with 8 specialized modes.

Author: ML Academic Framework
Version: 2.1
"""

import json
from pathlib import Path
from typing import Dict, Any, Optional

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from utils.file_manager import FileManager
from utils.input_validator import InputValidator
from utils.display_utils import DisplayUtils
from interface.generation_modes import GenerationModes
from src.data_processor import TextGenerator
from src.model import ModelManager


class Phase2GenerationInterface:
    """Handles Phase 2: Advanced text generation workflow."""
    
    def __init__(self, config):
        """Initialize generation interface."""
        self.config = config
        self.file_manager = FileManager()
        self.validator = InputValidator()
        self.display = DisplayUtils()
        self.generation_modes = GenerationModes()
    
    def run_generation_studio(self) -> bool:
        """Execute Phase 2 generation studio workflow."""
        self.display.clear_screen()
        print("ğŸ¨" * 15 + " FASE 2: ESTUDIO DE GENERACIÃ“N AVANZADO " + "ğŸ¨" * 15)
        print("=" * 80)
        print("ğŸ“ GENERACIÃ“N DE TEXTO CON MODELOS PRE-ENTRENADOS")
        print("ğŸ§  8 modos especializados para diferentes necesidades acadÃ©micas")
        print("=" * 80)
        
        # Step 1: List and select model
        models = self.file_manager.list_available_models_enhanced()
        if not models:
            self.display.show_error("No hay modelos entrenados disponibles")
            print("ğŸ’¡ Ejecuta primero FASE 1: Entrenamiento Intensivo")
            self.display.pause_for_user()
            return False
        
        selected_model = self._select_model(models)
        if not selected_model:
            return False
        
        # Step 2: Load model and metadata
        model, char_to_idx, idx_to_char, metadata = self._load_model_and_metadata(selected_model)
        if not model:
            return False
        
        # Step 3: Show model stats and enter generation loop
        self._show_model_summary(selected_model, metadata)
        return self._generation_loop(model, char_to_idx, idx_to_char, metadata, selected_model['name'])
    
    def _select_model(self, models: list) -> Optional[Dict]:
        """Display models and let user select one."""
        print("\nğŸ“Š MODELOS DISPONIBLES PARA GENERACIÃ“N")
        print("=" * 60)
        
        for i, model_info in enumerate(models, 1):
            print(f"{i}. ", end="")
            self.display.format_model_info(model_info)
        
        choice = self.validator.get_menu_choice("ğŸ¯ Selecciona modelo", len(models))
        return models[choice - 1]
    
    def _load_model_and_metadata(self, selected_model: Dict) -> tuple:
        """Load selected model and its metadata."""
        try:
            print(f"\nğŸ“š Cargando modelo: {selected_model['name']}")
            
            # Load model
            model_manager = ModelManager()
            model = model_manager.load_model(selected_model['path'])
            
            if not model:
                self.display.show_error("No se pudo cargar el modelo")
                return None, None, None, None
            
            # Load metadata and tokenizer info
            metadata = selected_model.get('metadata', {})
            if metadata:
                char_to_idx = metadata.get('char_to_idx', {})
                raw_idx_to_char = metadata.get('idx_to_char', {})
                # Ensure idx_to_char has integer keys
                idx_to_char = {int(k): v for k, v in raw_idx_to_char.items()}
            else:
                self.display.show_warning("Metadata no disponible, creando tokenizer bÃ¡sico")
                # Fallback tokenizer creation
                char_to_idx, idx_to_char = self._create_fallback_tokenizer()
            
            print("âœ… Modelo y metadata cargados exitosamente")
            return model, char_to_idx, idx_to_char, metadata
            
        except Exception as e:
            self.display.show_error(f"Error cargando modelo: {e}")
            return None, None, None, None
    
    def _create_fallback_tokenizer(self) -> tuple:
        """Create basic tokenizer from training text if metadata not available."""
        text_path = Path('The+48+Laws+Of+Power_texto.txt')
        if text_path.exists():
            with open(text_path, 'r', encoding='utf-8') as f:
                text = f.read()
            chars = sorted(set(text))
            char_to_idx = {c: i for i, c in enumerate(chars)}
            idx_to_char = {i: c for i, c in enumerate(chars)}
            return char_to_idx, idx_to_char
        else:
            self.display.show_error("No se puede crear tokenizer sin archivo de texto")
            return {}, {}
    
    def _show_model_summary(self, selected_model: Dict, metadata: Dict) -> None:
        """Display detailed model summary for academic purposes."""
        print(f"\nğŸ¤– MODELO SELECCIONADO: {selected_model['name']}")
        print("=" * 60)
        print(f"ğŸ“… Creado: {selected_model['created'].strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ’¾ TamaÃ±o: {selected_model['size_mb']:.1f} MB")
        print(f"â­ Calidad: {selected_model['quality_rating']}")
        
        if metadata:
            self.display.show_model_stats_summary(metadata)
        
        print("=" * 60)
        self.display.pause_for_user("Presiona Enter para continuar al estudio de generaciÃ³n...")
    
    def _generation_loop(self, model, char_to_idx: Dict, idx_to_char: Dict, 
                        metadata: Dict, model_name: str) -> bool:
        """Main generation loop with 8 specialized modes."""
        # Create text generator
        generator = TextGenerator(model, char_to_idx, idx_to_char)
        
        while True:
            self.display.clear_screen()
            self._show_generation_menu(model_name)
            
            choice = self.validator.get_menu_choice("ğŸ¯ Selecciona modo de generaciÃ³n", 8)
            
            try:
                if choice == 1:
                    self.generation_modes.quick_generation(generator)
                elif choice == 2:
                    self.generation_modes.creative_lab(generator)
                elif choice == 3:
                    self.generation_modes.interactive_session(generator)
                elif choice == 4:
                    self.generation_modes.batch_experiments(generator)
                elif choice == 5:
                    self.generation_modes.thematic_templates(generator)
                elif choice == 6:
                    self._show_detailed_analysis(metadata)
                elif choice == 7:
                    self._manage_generations()
                elif choice == 8:
                    print("ğŸ¯ Regresando al menÃº principal...")
                    return True
                
            except KeyboardInterrupt:
                print("\nâš ï¸ OperaciÃ³n interrumpida")
                if self.validator.get_confirmation("Â¿Regresar al menÃº principal?", True):
                    return True
            except Exception as e:
                self.display.show_error(f"Error en generaciÃ³n: {e}")
                self.display.pause_for_user()
    
    def _show_generation_menu(self, model_name: str) -> None:
        """Display the generation studio menu."""
        print("ğŸ¨ ESTUDIO DE GENERACIÃ“N AVANZADO")
        print("=" * 50)
        print(f"ğŸ¤– Modelo activo: {model_name}")
        print()
        print("ğŸ¯ MODOS DE GENERACIÃ“N:")
        print("1. ğŸš€ GeneraciÃ³n RÃ¡pida (presets optimizados)")
        print("2. ğŸ”¬ Laboratorio Creativo (control total)")
        print("3. ğŸ® SesiÃ³n Interactiva (comandos avanzados)")
        print("4. âš—ï¸ Experimentos en Lote (anÃ¡lisis sistemÃ¡tico)")
        print("5. ğŸ¨ Plantillas TemÃ¡ticas (estilos literarios)")
        print()
        print("ğŸ“Š ANÃLISIS Y GESTIÃ“N:")
        print("6. ğŸ“ˆ AnÃ¡lisis Detallado del Modelo")
        print("7. ğŸ“ Gestionar Generaciones Guardadas")
        print("8. ğŸšª Regresar al MenÃº Principal")
        print("=" * 50)
    
    def _show_detailed_analysis(self, metadata: Dict) -> None:
        """Show comprehensive model analysis."""
        print("\nğŸ“ˆ ANÃLISIS DETALLADO DEL MODELO")
        print("=" * 60)
        
        if not metadata:
            print("âš ï¸ Metadata no disponible para anÃ¡lisis detallado")
            self.display.pause_for_user()
            return
        
        # Training metrics
        print("ğŸ¯ MÃ‰TRICAS DE ENTRENAMIENTO:")
        print(f"   ğŸ“ˆ Ã‰pocas completadas: {metadata.get('final_epoch', 'N/A')}")
        print(f"   ğŸ“‰ Loss final: {metadata.get('final_loss', 'N/A'):.4f}" 
              if isinstance(metadata.get('final_loss'), (int, float)) else "   ğŸ“‰ Loss final: N/A")
        print(f"   ğŸ“Š Accuracy final: {metadata.get('final_accuracy', 'N/A'):.4f}" 
              if isinstance(metadata.get('final_accuracy'), (int, float)) else "   ğŸ“Š Accuracy final: N/A")
        print(f"   â±ï¸ DuraciÃ³n entrenamiento: {metadata.get('training_duration', 'N/A')}")
        
        # Model architecture
        print(f"\nğŸ§  ARQUITECTURA:")
        print(f"   ğŸ¯ Vocabulario: {metadata.get('vocab_size', 'N/A')} caracteres Ãºnicos")
        print(f"   ğŸ“ Longitud de secuencia: {metadata.get('sequence_length', 'N/A')} tokens")
        print(f"   ğŸ§  LSTM units: {metadata.get('lstm_units', 'N/A')}")
        print(f"   ğŸ’§ Dropout rate: {metadata.get('dropout_rate', 'N/A')}")
        print(f"   ğŸ“¦ Batch size: {metadata.get('batch_size', 'N/A')}")
        
        # Performance recommendations
        final_loss = metadata.get('final_loss')
        if isinstance(final_loss, (int, float)):
            print(f"\nğŸ¯ RECOMENDACIONES DE USO:")
            if final_loss < 1.0:
                print("   ğŸŒŸ Excelente para generaciÃ³n creativa y narrativa")
                print("   ğŸ“ Usa temperature 0.7-1.0 para mejores resultados")
                print("   ğŸ“ Longitudes 200-500 caracteres son Ã³ptimas")
            elif final_loss < 1.5:
                print("   â­ Bueno para textos coherentes y experimentaciÃ³n")
                print("   ğŸ“ Usa temperature 0.6-0.9 para estabilidad")
                print("   ğŸ“ Longitudes 150-300 caracteres recomendadas")
            elif final_loss < 2.0:
                print("   ğŸ“Š Aceptable para pruebas y aprendizaje")
                print("   ğŸ“ Usa temperature 0.5-0.7 para mayor coherencia")
                print("   ğŸ“ Longitudes cortas (100-200) son mÃ¡s estables")
            else:
                print("   âš ï¸ Modelo requiere mÃ¡s entrenamiento")
                print("   ğŸ“ Usa temperature baja (0.4-0.6) y longitudes cortas")
                print("   ğŸ”„ Considera entrenar mÃ¡s Ã©pocas")
        
        print("=" * 60)
        self.display.pause_for_user()
    
    def _manage_generations(self) -> None:
        """Manage saved generations."""
        print("\nğŸ“ GESTIONAR GENERACIONES GUARDADAS")
        print("=" * 50)
        
        generations = self.file_manager.get_available_generations()
        
        if not generations:
            print("ğŸ“­ No hay generaciones guardadas")
            self.display.pause_for_user()
            return
        
        print(f"ğŸ“Š Total de generaciones guardadas: {len(generations)}")
        print()
        
        for i, gen in enumerate(generations[:10], 1):  # Show last 10
            print(f"{i}. {gen['timestamp']} ({gen['size_kb']:.1f} KB)")
            if gen['metadata']:
                meta = gen['metadata']
                print(f"   ğŸŒ± Seed: \"{meta.get('seed', 'N/A')}\"")
                print(f"   ğŸŒ¡ï¸ T={meta.get('temperature', 'N/A')} | L={meta.get('length', 'N/A')}")
            print()
        
        if len(generations) > 10:
            print(f"   ... y {len(generations) - 10} mÃ¡s")
        
        print("\nğŸ’¡ Las generaciones se guardan en el directorio 'generations/'")
        self.display.pause_for_user()